#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®æµ‹è¯•æ–‡ä»¶è¿›è¡Œæ‰‹éƒ¨æ´—æ‰‹æ£€æµ‹æµ‹è¯•
ä½¿ç”¨ tests/fixtures ç›®å½•ä¸‹çš„å®é™…å›¾ç‰‡å’Œè§†é¢‘æ–‡ä»¶
"""

import os
import time

import cv2
import numpy as np

from src.core.behavior import BehaviorRecognizer
from src.core.detector import HumanDetector
from src.core.pose_detector import PoseDetector
from src.services.detection_service import DetectionResult


def process_image(image_path, detectors):
    """å¤„ç†å•å¼ å›¾ç‰‡"""
    pose_detector, behavior_recognizer, human_detector = detectors

    print(f"\nğŸ“¸ å¤„ç†å›¾ç‰‡: {os.path.basename(image_path)}")

    # è¯»å–å›¾ç‰‡
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {image_path}")
        return None

    print(f"   å›¾ç‰‡å°ºå¯¸: {image.shape[1]}x{image.shape[0]}")

    try:
        # äººä½“æ£€æµ‹
        person_detections = human_detector.detect(image)
        print(f"   æ£€æµ‹åˆ°äººä½“: {len(person_detections)} ä¸ª")

        # åˆ›å»ºæ£€æµ‹ç»“æœå¯¹è±¡
        result = DetectionResult(
            person_detections=person_detections,
            hairnet_results=[],
            handwash_results=[],
            sanitize_results=[],
            processing_times={"total": 0.0},
        )

        # åˆ›å»ºæ ‡æ³¨å›¾åƒ
        annotated_image = image.copy()
        hands_count = 0

        # åªæœ‰åœ¨æ£€æµ‹åˆ°äººä½“æ—¶æ‰è¿›è¡Œæ‰‹éƒ¨æ£€æµ‹
        if person_detections:
            print("   ğŸ” æ‰§è¡Œæ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹...")

            # æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹
            hands_results = pose_detector.detect_hands(image)
            hands_count = len(hands_results)
            print(f"   æ£€æµ‹åˆ°æ‰‹éƒ¨: {hands_count} ä¸ª")

            # æ´—æ‰‹è¡Œä¸ºæ£€æµ‹
            for i, person in enumerate(person_detections):
                person_bbox = person.get("bbox", [])
                if len(person_bbox) >= 4:
                    # ä½¿ç”¨è¡Œä¸ºè¯†åˆ«å™¨æ£€æµ‹æ´—æ‰‹åŠ¨ä½œ
                    handwash_confidence = behavior_recognizer.detect_handwashing(
                        person_bbox, hands_results, track_id=i, frame=image
                    )

                    handwash_result = {
                        "is_handwashing": handwash_confidence
                        > behavior_recognizer.confidence_threshold,
                        "confidence": handwash_confidence,
                    }

                    if handwash_result:
                        result.handwash_results.append(
                            {
                                "person_bbox": person_bbox,
                                "is_handwashing": handwash_result.get(
                                    "is_handwashing", False
                                ),
                                "confidence": handwash_result.get("confidence", 0.0),
                            }
                        )

            # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
            for hand_result in hands_results:
                landmarks = hand_result["landmarks"]
                hand_label = hand_result["label"]
                bbox = hand_result["bbox"]

                # ç»˜åˆ¶æ‰‹éƒ¨è¾¹ç•Œæ¡†
                cv2.rectangle(
                    annotated_image,
                    (bbox[0], bbox[1]),
                    (bbox[2], bbox[3]),
                    (255, 255, 0),
                    2,
                )  # é»„è‰²è¾¹ç•Œæ¡†

                # ç»˜åˆ¶æ‰‹éƒ¨æ ‡ç­¾
                cv2.putText(
                    annotated_image,
                    f"Hand: {hand_label}",
                    (bbox[0], bbox[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    (255, 255, 0),
                    2,
                )

                # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
                h, w = annotated_image.shape[:2]
                for i, landmark in enumerate(landmarks):
                    x = int(landmark["x"] * w)
                    y = int(landmark["y"] * h)

                    # ç»˜åˆ¶å…³é”®ç‚¹
                    cv2.circle(annotated_image, (x, y), 3, (0, 255, 255), -1)  # é’è‰²åœ†ç‚¹

                    # ä¸ºé‡è¦å…³é”®ç‚¹æ·»åŠ æ ‡ç­¾
                    if i in [4, 8, 12, 16, 20, 0]:  # MediaPipeæ‰‹éƒ¨å…³é”®ç‚¹ç´¢å¼•
                        point_names = {
                            0: "è…•",
                            4: "æ‹‡æŒ‡",
                            8: "é£ŸæŒ‡",
                            12: "ä¸­æŒ‡",
                            16: "æ— åæŒ‡",
                            20: "å°æŒ‡",
                        }
                        if i in point_names:
                            cv2.putText(
                                annotated_image,
                                point_names[i],
                                (x + 5, y - 5),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                0.3,
                                (0, 255, 255),
                                1,
                            )

                # ç»˜åˆ¶æ‰‹éƒ¨è¿æ¥çº¿
                if len(landmarks) >= 21:
                    # è¿æ¥æ‰‹è…•åˆ°å„æŒ‡æ ¹éƒ¨
                    wrist = (int(landmarks[0]["x"] * w), int(landmarks[0]["y"] * h))
                    finger_bases = [5, 9, 13, 17]
                    for base_idx in finger_bases:
                        if base_idx < len(landmarks):
                            base = (
                                int(landmarks[base_idx]["x"] * w),
                                int(landmarks[base_idx]["y"] * h),
                            )
                            cv2.line(annotated_image, wrist, base, (0, 255, 255), 1)

                    # è¿æ¥å„æŒ‡å…³èŠ‚
                    finger_connections = [
                        [1, 2, 3, 4],  # æ‹‡æŒ‡
                        [5, 6, 7, 8],  # é£ŸæŒ‡
                        [9, 10, 11, 12],  # ä¸­æŒ‡
                        [13, 14, 15, 16],  # æ— åæŒ‡
                        [17, 18, 19, 20],  # å°æŒ‡
                    ]

                    for finger in finger_connections:
                        for j in range(len(finger) - 1):
                            if finger[j] < len(landmarks) and finger[j + 1] < len(
                                landmarks
                            ):
                                pt1 = (
                                    int(landmarks[finger[j]]["x"] * w),
                                    int(landmarks[finger[j]]["y"] * h),
                                )
                                pt2 = (
                                    int(landmarks[finger[j + 1]]["x"] * w),
                                    int(landmarks[finger[j + 1]]["y"] * h),
                                )
                                cv2.line(annotated_image, pt1, pt2, (0, 255, 255), 1)

        # ç»˜åˆ¶äººä½“æ£€æµ‹æ¡†
        for person in person_detections:
            bbox = person.get("bbox", [])
            if len(bbox) >= 4:
                x1, y1, x2, y2 = map(int, bbox[:4])
                confidence = person.get("confidence", 0)

                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(annotated_image, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f"Person {confidence:.2f}"
                cv2.putText(
                    annotated_image,
                    label,
                    (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    (0, 255, 0),
                    2,
                )

        # ç»˜åˆ¶æ´—æ‰‹æ£€æµ‹ç»“æœ
        for handwash_result in result.handwash_results:
            if handwash_result.get("is_handwashing", False):
                person_bbox = handwash_result.get("person_bbox", [])
                if len(person_bbox) >= 4:
                    x1, y1, x2, y2 = map(int, person_bbox[:4])
                    confidence = handwash_result.get("confidence", 0)

                    # ç»˜åˆ¶æ´—æ‰‹æ ‡ç­¾
                    cv2.putText(
                        annotated_image,
                        f"æ´—æ‰‹ä¸­ {confidence:.2f}",
                        (x1, y1 - 30),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.6,
                        (0, 255, 255),
                        2,
                    )

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        info_lines = [
            f"æ–‡ä»¶: {os.path.basename(image_path)}",
            f"äººä½“: {len(person_detections)}",
            f"æ‰‹éƒ¨: {hands_count}",
            f"æ´—æ‰‹: {len([r for r in result.handwash_results if r.get('is_handwashing', False)])}",
        ]

        for i, info in enumerate(info_lines):
            cv2.putText(
                annotated_image,
                info,
                (10, 30 + i * 25),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 255, 255),
                2,
            )

        # æ·»åŠ æ“ä½œæç¤º
        cv2.putText(
            annotated_image,
            "æŒ‰ä»»æ„é”®ç»§ç»­, æŒ‰ 'q' é€€å‡º, æŒ‰ 's' ä¿å­˜",
            (10, annotated_image.shape[0] - 20),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (255, 255, 255),
            1,
        )

        return annotated_image, result

    except Exception as e:
        print(f"   âš ï¸ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return None, None


def process_video(video_path, detectors, max_frames=100):
    """å¤„ç†è§†é¢‘æ–‡ä»¶"""
    pose_detector, behavior_recognizer, human_detector = detectors

    print(f"\nğŸ¥ å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")

    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        return

    # è·å–è§†é¢‘ä¿¡æ¯
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    print(f"   è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.1f}fps, {frame_count}å¸§")
    print(f"   å°†å¤„ç†å‰ {min(max_frames, frame_count)} å¸§")

    frame_idx = 0
    processed_frames = 0

    while True:
        ret, frame = cap.read()
        if not ret or processed_frames >= max_frames:
            break

        frame_idx += 1

        # æ¯éš”å‡ å¸§å¤„ç†ä¸€æ¬¡ä»¥æé«˜æ€§èƒ½
        if frame_idx % 5 != 0:
            continue

        processed_frames += 1

        try:
            # äººä½“æ£€æµ‹
            person_detections = human_detector.detect(frame)
            hands_count = 0

            # åªæœ‰åœ¨æ£€æµ‹åˆ°äººä½“æ—¶æ‰è¿›è¡Œæ‰‹éƒ¨æ£€æµ‹
            if person_detections:
                # æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹
                hands_results = pose_detector.detect_hands(frame)
                hands_count = len(hands_results)

                # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
                for hand_result in hands_results:
                    landmarks = hand_result["landmarks"]
                    hand_label = hand_result["label"]
                    bbox = hand_result["bbox"]

                    # ç»˜åˆ¶æ‰‹éƒ¨è¾¹ç•Œæ¡†
                    cv2.rectangle(
                        frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 0), 2
                    )

                    # ç»˜åˆ¶æ‰‹éƒ¨æ ‡ç­¾
                    cv2.putText(
                        frame,
                        f"Hand: {hand_label}",
                        (bbox[0], bbox[1] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5,
                        (255, 255, 0),
                        2,
                    )

                    # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
                    h, w = frame.shape[:2]
                    for i, landmark in enumerate(landmarks):
                        x = int(landmark["x"] * w)
                        y = int(landmark["y"] * h)

                        # ç»˜åˆ¶å…³é”®ç‚¹
                        cv2.circle(frame, (x, y), 2, (0, 255, 255), -1)

                        # ä¸ºé‡è¦å…³é”®ç‚¹æ·»åŠ æ ‡ç­¾
                        if i in [0, 4, 8, 12, 16, 20]:
                            point_names = {
                                0: "è…•",
                                4: "æ‹‡æŒ‡",
                                8: "é£ŸæŒ‡",
                                12: "ä¸­æŒ‡",
                                16: "æ— åæŒ‡",
                                20: "å°æŒ‡",
                            }
                            if i in point_names:
                                cv2.putText(
                                    frame,
                                    point_names[i],
                                    (x + 3, y - 3),
                                    cv2.FONT_HERSHEY_SIMPLEX,
                                    0.3,
                                    (0, 255, 255),
                                    1,
                                )

            # ç»˜åˆ¶äººä½“æ£€æµ‹æ¡†
            for person in person_detections:
                bbox = person.get("bbox", [])
                if len(bbox) >= 4:
                    x1, y1, x2, y2 = map(int, bbox[:4])
                    confidence = person.get("confidence", 0)

                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

                    # ç»˜åˆ¶æ ‡ç­¾
                    label = f"Person {confidence:.2f}"
                    cv2.putText(
                        frame,
                        label,
                        (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5,
                        (0, 255, 0),
                        2,
                    )

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            info_lines = [
                f"å¸§: {frame_idx}/{frame_count}",
                f"äººä½“: {len(person_detections)}",
                f"æ‰‹éƒ¨: {hands_count}",
            ]

            for i, info in enumerate(info_lines):
                cv2.putText(
                    frame,
                    info,
                    (10, 30 + i * 25),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 255),
                    2,
                )

            # æ·»åŠ æ“ä½œæç¤º
            cv2.putText(
                frame,
                "æŒ‰ 'q' é€€å‡º, æŒ‰ 's' æˆªå›¾",
                (10, frame.shape[0] - 20),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (255, 255, 255),
                1,
            )

        except Exception as e:
            print(f"   âš ï¸ å¤„ç†ç¬¬{frame_idx}å¸§æ—¶å‡ºé”™: {e}")

        # æ˜¾ç¤ºå¸§
        cv2.imshow(f"è§†é¢‘æ£€æµ‹ - {os.path.basename(video_path)}", frame)

        # å¤„ç†æŒ‰é”®
        key = cv2.waitKey(1) & 0xFF
        if key == ord("q"):
            print("   ğŸ‘‹ ç”¨æˆ·é€€å‡ºè§†é¢‘æ’­æ”¾")
            break
        elif key == ord("s"):
            # ä¿å­˜æˆªå›¾
            timestamp = int(time.time())
            filename = f"video_frame_{timestamp}.jpg"
            cv2.imwrite(filename, frame)
            print(f"   ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {filename}")

    cap.release()
    cv2.destroyAllWindows()
    print(f"   âœ… è§†é¢‘å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_frames} å¸§")


def main():
    print("ğŸš€ å¯åŠ¨çœŸå®æµ‹è¯•æ–‡ä»¶æ‰‹éƒ¨æ´—æ‰‹æ£€æµ‹")
    print("ä½¿ç”¨ tests/fixtures ç›®å½•ä¸‹çš„å®é™…å›¾ç‰‡å’Œè§†é¢‘æ–‡ä»¶")

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    try:
        pose_detector = PoseDetector()
        behavior_recognizer = BehaviorRecognizer()
        human_detector = HumanDetector()
        detectors = (pose_detector, behavior_recognizer, human_detector)
        print("âœ… æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    fixtures_dir = "/Users/zhou/Code/python/Pyt/tests/fixtures"
    image_files = [
        os.path.join(fixtures_dir, "images/person/test_person.png"),
        os.path.join(fixtures_dir, "images/hairnet/7æœˆ23æ—¥.png"),
    ]
    video_files = [
        os.path.join(fixtures_dir, "videos/20250724072708.mp4"),
        os.path.join(fixtures_dir, "videos/20250724072822_175680.mp4"),
    ]

    print("\nğŸ“‹ æµ‹è¯•è®¡åˆ’:")
    print(f"  - å›¾ç‰‡æ–‡ä»¶: {len([f for f in image_files if os.path.exists(f)])} ä¸ª")
    print(f"  - è§†é¢‘æ–‡ä»¶: {len([f for f in video_files if os.path.exists(f)])} ä¸ª")

    # å¤„ç†å›¾ç‰‡æ–‡ä»¶
    print("\nğŸ–¼ï¸ ===== å›¾ç‰‡æ£€æµ‹æµ‹è¯• =====")
    for image_path in image_files:
        if os.path.exists(image_path):
            annotated_image, result = process_image(image_path, detectors)

            if annotated_image is not None:
                # ä¿å­˜æ ‡æ³¨åçš„å›¾åƒ
                output_filename = f"annotated_{os.path.basename(image_path)}"
                cv2.imwrite(output_filename, annotated_image)
                print(f"   ğŸ’¾ æ ‡æ³¨å›¾åƒå·²ä¿å­˜: {output_filename}")

                # æ˜¾ç¤ºå›¾åƒ
                cv2.imshow(f"æ£€æµ‹ç»“æœ - {os.path.basename(image_path)}", annotated_image)

                print(f"\n   ğŸ“Š æ£€æµ‹ç»“æœç»Ÿè®¡:")
                if result:
                    print(f"     - æ£€æµ‹åˆ°äººä½“: {len(result.person_detections)} ä¸ª")
                    hands_count = 0
                    try:
                        hands_results = pose_detector.detect_hands(
                            cv2.imread(image_path)
                        )
                        hands_count = len(hands_results)
                    except:
                        pass
                    print(f"     - æ£€æµ‹åˆ°æ‰‹éƒ¨: {hands_count} ä¸ª")
                    print(
                        f"     - æ´—æ‰‹è¡Œä¸º: {len([r for r in result.handwash_results if r.get('is_handwashing', False)])} ä¸ª"
                    )

                print("\n   æŒ‰ä»»æ„é”®ç»§ç»­ä¸‹ä¸€å¼ å›¾ç‰‡ï¼ŒæŒ‰ 'q' è·³è¿‡å›¾ç‰‡æµ‹è¯•...")
                key = cv2.waitKey(0) & 0xFF
                cv2.destroyAllWindows()

                if key == ord("q"):
                    print("   ğŸ‘‹ è·³è¿‡å‰©ä½™å›¾ç‰‡æµ‹è¯•")
                    break
        else:
            print(f"   âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

    # è¯¢é—®æ˜¯å¦ç»§ç»­è§†é¢‘æµ‹è¯•
    print("\nğŸ¥ ===== è§†é¢‘æ£€æµ‹æµ‹è¯• =====")
    print("æ˜¯å¦ç»§ç»­è¿›è¡Œè§†é¢‘æ£€æµ‹æµ‹è¯•ï¼Ÿ(y/n): ", end="")
    choice = input().lower().strip()

    if choice == "y" or choice == "yes":
        # å¤„ç†è§†é¢‘æ–‡ä»¶
        for video_path in video_files:
            if os.path.exists(video_path):
                process_video(video_path, detectors, max_frames=50)  # é™åˆ¶å¤„ç†å¸§æ•°

                print("\nç»§ç»­ä¸‹ä¸€ä¸ªè§†é¢‘ï¼Ÿ(y/n): ", end="")
                choice = input().lower().strip()
                if choice != "y" and choice != "yes":
                    print("ğŸ‘‹ è·³è¿‡å‰©ä½™è§†é¢‘æµ‹è¯•")
                    break
            else:
                print(f"âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
    else:
        print("ğŸ‘‹ è·³è¿‡è§†é¢‘æµ‹è¯•")

    print("\nâœ… æµ‹è¯•å®Œæˆ")
    print("\nğŸ’¡ åŠŸèƒ½ç‰¹ç‚¹:")
    print("  - âœ… ä½¿ç”¨çœŸå®æµ‹è¯•æ–‡ä»¶è¿›è¡Œæ£€æµ‹")
    print("  - âœ… åªåœ¨æ£€æµ‹åˆ°äººä½“æ—¶æ˜¾ç¤ºæ‰‹éƒ¨å…³é”®ç‚¹")
    print("  - âœ… æ”¯æŒå¤šæ‰‹æ£€æµ‹å’Œå·¦å³æ‰‹åŒºåˆ†")
    print("  - âœ… 21ä¸ªå…³é”®ç‚¹æ ‡æ³¨")
    print("  - âœ… æ˜¾ç¤ºæ‰‹éƒ¨è¾¹ç•Œæ¡†å’Œé‡è¦å…³é”®ç‚¹æ ‡ç­¾")
    print("  - âœ… ç»˜åˆ¶æ‰‹éƒ¨éª¨æ¶è¿æ¥çº¿")
    print("  - âœ… å®æ—¶æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡ä¿¡æ¯")
    print("  - âœ… æ”¯æŒå›¾ç‰‡å’Œè§†é¢‘æ–‡ä»¶å¤„ç†")
    print("  - âœ… è‡ªåŠ¨ä¿å­˜æ ‡æ³¨ç»“æœ")


if __name__ == "__main__":
    main()
