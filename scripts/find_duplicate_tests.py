#!/usr/bin/env python3
"""
é‡å¤æµ‹è¯•æ£€æµ‹è„šæœ¬

åˆ†æé¡¹ç›®ä¸­åŠŸèƒ½é‡å¤çš„æµ‹è¯•ç”¨ä¾‹
"""

import subprocess
import re
from collections import defaultdict, Counter
from pathlib import Path
from typing import List, Dict, Set, Tuple
import ast
import inspect

# è·å–é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).parent.parent

def extract_test_functions_from_file(file_path: Path) -> List[Dict]:
    """
    ä»æµ‹è¯•æ–‡ä»¶ä¸­æå–æµ‹è¯•å‡½æ•°ä¿¡æ¯
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£æAST
        tree = ast.parse(content)
        
        test_functions = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
                # æå–å‡½æ•°ä¿¡æ¯
                func_info = {
                    'name': node.name,
                    'file': str(file_path),
                    'line': node.lineno,
                    'docstring': ast.get_docstring(node) or '',
                    'args': [arg.arg for arg in node.args.args],
                    'body_lines': node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                }
                
                # æå–å‡½æ•°ä½“ä¸­çš„å…³é”®è¯
                func_info['keywords'] = extract_keywords_from_node(node)
                
                test_functions.append(func_info)
        
        return test_functions
    
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
        return []

def extract_keywords_from_node(node: ast.FunctionDef) -> Set[str]:
    """
    ä»ASTèŠ‚ç‚¹ä¸­æå–å…³é”®è¯
    """
    keywords = set()
    
    for child in ast.walk(node):
        # æå–å‡½æ•°è°ƒç”¨
        if isinstance(child, ast.Call):
            if isinstance(child.func, ast.Name):
                keywords.add(child.func.id)
            elif isinstance(child.func, ast.Attribute):
                keywords.add(child.func.attr)
        
        # æå–å˜é‡å
        elif isinstance(child, ast.Name):
            keywords.add(child.id)
        
        # æå–å­—ç¬¦ä¸²å¸¸é‡
        elif isinstance(child, ast.Constant) and isinstance(child.value, str):
            # åªä¿ç•™æœ‰æ„ä¹‰çš„å­—ç¬¦ä¸²ï¼ˆé•¿åº¦>3ä¸”ä¸æ˜¯è·¯å¾„ï¼‰
            if len(child.value) > 3 and '/' not in child.value and '\\' not in child.value:
                keywords.add(child.value.lower())
    
    return keywords

def normalize_test_name(test_name: str) -> str:
    """
    æ ‡å‡†åŒ–æµ‹è¯•åç§°ï¼Œç”¨äºæ¯”è¾ƒç›¸ä¼¼æ€§
    """
    # ç§»é™¤test_å‰ç¼€
    name = test_name.replace('test_', '')
    
    # ç§»é™¤å¸¸è§çš„å˜ä½“è¯æ±‡
    variations = {
        'with_': '',
        'without_': '',
        'when_': '',
        'should_': '',
        'can_': '',
        'will_': '',
        'invalid_': 'bad_',
        'valid_': 'good_',
        'successful_': 'success_',
        'failed_': 'failure_',
        'empty_': 'none_',
        'null_': 'none_',
    }
    
    for old, new in variations.items():
        name = name.replace(old, new)
    
    return name

def calculate_similarity(func1: Dict, func2: Dict) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæµ‹è¯•å‡½æ•°çš„ç›¸ä¼¼åº¦
    """
    # åç§°ç›¸ä¼¼åº¦
    name1 = normalize_test_name(func1['name'])
    name2 = normalize_test_name(func2['name'])
    
    name_similarity = calculate_string_similarity(name1, name2)
    
    # å…³é”®è¯ç›¸ä¼¼åº¦
    keywords1 = func1['keywords']
    keywords2 = func2['keywords']
    
    if not keywords1 and not keywords2:
        keyword_similarity = 0.0
    elif not keywords1 or not keywords2:
        keyword_similarity = 0.0
    else:
        common_keywords = keywords1.intersection(keywords2)
        total_keywords = keywords1.union(keywords2)
        keyword_similarity = len(common_keywords) / len(total_keywords) if total_keywords else 0.0
    
    # æ–‡æ¡£å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    doc1 = func1['docstring'].lower()
    doc2 = func2['docstring'].lower()
    doc_similarity = calculate_string_similarity(doc1, doc2) if doc1 and doc2 else 0.0
    
    # å‚æ•°ç›¸ä¼¼åº¦
    args1 = set(func1['args'])
    args2 = set(func2['args'])
    
    if not args1 and not args2:
        args_similarity = 1.0
    elif not args1 or not args2:
        args_similarity = 0.0
    else:
        common_args = args1.intersection(args2)
        total_args = args1.union(args2)
        args_similarity = len(common_args) / len(total_args) if total_args else 0.0
    
    # åŠ æƒè®¡ç®—æ€»ç›¸ä¼¼åº¦
    total_similarity = (
        name_similarity * 0.4 +
        keyword_similarity * 0.3 +
        doc_similarity * 0.2 +
        args_similarity * 0.1
    )
    
    return total_similarity

def calculate_string_similarity(s1: str, s2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„Jaccardç›¸ä¼¼åº¦ï¼‰
    """
    if not s1 and not s2:
        return 1.0
    if not s1 or not s2:
        return 0.0
    
    # åˆ†è¯
    words1 = set(re.findall(r'\w+', s1.lower()))
    words2 = set(re.findall(r'\w+', s2.lower()))
    
    if not words1 and not words2:
        return 1.0
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0.0

def find_duplicate_tests() -> Dict:
    """
    æŸ¥æ‰¾é‡å¤çš„æµ‹è¯•ç”¨ä¾‹
    """
    print("æ­£åœ¨æ‰«ææµ‹è¯•æ–‡ä»¶...")
    
    # æ”¶é›†æ‰€æœ‰æµ‹è¯•æ–‡ä»¶
    test_files = []
    
    # æ‰«ætestsç›®å½•
    tests_dir = ROOT_DIR / 'tests'
    if tests_dir.exists():
        test_files.extend(tests_dir.rglob('test_*.py'))
    
    # æ‰«ææ ¹ç›®å½•çš„æµ‹è¯•æ–‡ä»¶
    test_files.extend(ROOT_DIR.glob('test_*.py'))
    
    print(f"æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    
    # æå–æ‰€æœ‰æµ‹è¯•å‡½æ•°
    all_test_functions = []
    
    for test_file in test_files:
        functions = extract_test_functions_from_file(test_file)
        all_test_functions.extend(functions)
    
    print(f"æ‰¾åˆ° {len(all_test_functions)} ä¸ªæµ‹è¯•å‡½æ•°")
    
    # æŸ¥æ‰¾é‡å¤
    duplicates = []
    similarity_threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
    
    for i, func1 in enumerate(all_test_functions):
        for j, func2 in enumerate(all_test_functions[i+1:], i+1):
            # è·³è¿‡åŒä¸€ä¸ªæ–‡ä»¶ä¸­çš„å‡½æ•°ï¼ˆå¯èƒ½æ˜¯åˆç†çš„é‡è½½ï¼‰
            if func1['file'] == func2['file']:
                continue
            
            similarity = calculate_similarity(func1, func2)
            
            if similarity >= similarity_threshold:
                duplicates.append({
                    'func1': func1,
                    'func2': func2,
                    'similarity': similarity
                })
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    duplicates.sort(key=lambda x: x['similarity'], reverse=True)
    
    # åˆ†æç»“æœ
    analysis = {
        'total_functions': len(all_test_functions),
        'total_files': len(test_files),
        'duplicates': duplicates,
        'duplicate_count': len(duplicates),
        'functions_by_file': defaultdict(list)
    }
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„
    for func in all_test_functions:
        file_name = Path(func['file']).name
        analysis['functions_by_file'][file_name].append(func)
    
    return analysis

def print_duplicate_analysis(analysis: Dict):
    """
    æ‰“å°é‡å¤æµ‹è¯•åˆ†æç»“æœ
    """
    print("\n" + "="*60)
    print("ğŸ” é‡å¤æµ‹è¯•æ£€æµ‹æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"æµ‹è¯•æ–‡ä»¶æ€»æ•°: {analysis['total_files']}")
    print(f"æµ‹è¯•å‡½æ•°æ€»æ•°: {analysis['total_functions']}")
    print(f"ç–‘ä¼¼é‡å¤æµ‹è¯•å¯¹: {analysis['duplicate_count']}")
    
    if analysis['duplicate_count'] == 0:
        print("\nâœ… æœªå‘ç°æ˜æ˜¾çš„é‡å¤æµ‹è¯•ï¼")
        return
    
    print(f"\nğŸš¨ å‘ç°çš„é‡å¤æµ‹è¯• (ç›¸ä¼¼åº¦ â‰¥ 70%):")
    
    for i, dup in enumerate(analysis['duplicates'], 1):
        func1 = dup['func1']
        func2 = dup['func2']
        similarity = dup['similarity']
        
        print(f"\n  {i}. ç›¸ä¼¼åº¦: {similarity:.1%}")
        print(f"     ğŸ“„ {Path(func1['file']).name}:{func1['line']} - {func1['name']}")
        print(f"     ğŸ“„ {Path(func2['file']).name}:{func2['line']} - {func2['name']}")
        
        # æ˜¾ç¤ºæ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆå¦‚æœæœ‰ï¼‰
        if func1['docstring']:
            print(f"        æè¿°1: {func1['docstring'][:50]}...")
        if func2['docstring']:
            print(f"        æè¿°2: {func2['docstring'][:50]}...")
        
        # æ˜¾ç¤ºå…±åŒå…³é”®è¯
        common_keywords = func1['keywords'].intersection(func2['keywords'])
        if common_keywords:
            keywords_str = ', '.join(sorted(list(common_keywords))[:5])
            print(f"        å…±åŒå…³é”®è¯: {keywords_str}")
    
    # æŒ‰æ–‡ä»¶åˆ†æé‡å¤æƒ…å†µ
    print(f"\nğŸ“ æŒ‰æ–‡ä»¶åˆ†æé‡å¤æƒ…å†µ:")
    
    file_duplicate_count = defaultdict(int)
    for dup in analysis['duplicates']:
        file1 = Path(dup['func1']['file']).name
        file2 = Path(dup['func2']['file']).name
        file_duplicate_count[file1] += 1
        file_duplicate_count[file2] += 1
    
    for file_name, count in sorted(file_duplicate_count.items(), key=lambda x: x[1], reverse=True):
        print(f"  {file_name}: {count} ä¸ªé‡å¤æµ‹è¯•")
    
    # æä¾›å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    high_similarity = [d for d in analysis['duplicates'] if d['similarity'] > 0.9]
    if high_similarity:
        print(f"  1. é«˜åº¦ç›¸ä¼¼æµ‹è¯• ({len(high_similarity)} å¯¹): è€ƒè™‘åˆå¹¶æˆ–é‡æ„")
        for dup in high_similarity[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            func1_name = Path(dup['func1']['file']).name
            func2_name = Path(dup['func2']['file']).name
            print(f"     - {func1_name}::{dup['func1']['name']} vs {func2_name}::{dup['func2']['name']}")
    
    medium_similarity = [d for d in analysis['duplicates'] if 0.7 <= d['similarity'] <= 0.9]
    if medium_similarity:
        print(f"  2. ä¸­åº¦ç›¸ä¼¼æµ‹è¯• ({len(medium_similarity)} å¯¹): æ£€æŸ¥æ˜¯å¦å¯ä»¥æå–å…¬å…±æµ‹è¯•é€»è¾‘")
    
    # æ£€æŸ¥æµ‹è¯•å‘½åæ¨¡å¼
    name_patterns = defaultdict(list)
    for func in [f for file_funcs in analysis['functions_by_file'].values() for f in file_funcs]:
        # æå–å‘½åæ¨¡å¼
        name_parts = func['name'].replace('test_', '').split('_')
        if len(name_parts) > 1:
            pattern = '_'.join(name_parts[:-1])  # é™¤äº†æœ€åä¸€éƒ¨åˆ†
            name_patterns[pattern].append(func)
    
    similar_patterns = {k: v for k, v in name_patterns.items() if len(v) > 2}
    if similar_patterns:
        print(f"  3. ç›¸ä¼¼å‘½åæ¨¡å¼: æ£€æŸ¥ä»¥ä¸‹æµ‹è¯•æ˜¯å¦å¯ä»¥å‚æ•°åŒ–")
        for pattern, funcs in list(similar_patterns.items())[:3]:
            print(f"     - æ¨¡å¼ '{pattern}_*': {len(funcs)} ä¸ªæµ‹è¯•")

def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        analysis = find_duplicate_tests()
        print_duplicate_analysis(analysis)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
        report_file = ROOT_DIR / 'reports' / 'duplicate_tests_report.md'
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# é‡å¤æµ‹è¯•æ£€æµ‹æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- æµ‹è¯•æ–‡ä»¶æ€»æ•°: {analysis['total_files']}\n")
            f.write(f"- æµ‹è¯•å‡½æ•°æ€»æ•°: {analysis['total_functions']}\n")
            f.write(f"- ç–‘ä¼¼é‡å¤æµ‹è¯•å¯¹: {analysis['duplicate_count']}\n\n")
            
            if analysis['duplicates']:
                f.write("## é‡å¤æµ‹è¯•è¯¦æƒ…\n\n")
                for i, dup in enumerate(analysis['duplicates'], 1):
                    func1 = dup['func1']
                    func2 = dup['func2']
                    similarity = dup['similarity']
                    
                    f.write(f"### {i}. ç›¸ä¼¼åº¦: {similarity:.1%}\n\n")
                    f.write(f"**æµ‹è¯•1:** `{Path(func1['file']).name}:{func1['line']}` - `{func1['name']}`\n")
                    f.write(f"**æµ‹è¯•2:** `{Path(func2['file']).name}:{func2['line']}` - `{func2['name']}`\n\n")
                    
                    if func1['docstring']:
                        f.write(f"**æè¿°1:** {func1['docstring']}\n\n")
                    if func2['docstring']:
                        f.write(f"**æè¿°2:** {func2['docstring']}\n\n")
                    
                    common_keywords = func1['keywords'].intersection(func2['keywords'])
                    if common_keywords:
                        f.write(f"**å…±åŒå…³é”®è¯:** {', '.join(sorted(common_keywords))}\n\n")
                    
                    f.write("---\n\n")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()