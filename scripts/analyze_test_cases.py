#!/usr/bin/env python3
"""
æµ‹è¯•ç”¨ä¾‹åˆ†æè„šæœ¬

åˆ†æé¡¹ç›®ä¸­æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„åˆ†å¸ƒå’ŒåŠŸèƒ½
"""

import subprocess
import re
from collections import defaultdict, Counter
from pathlib import Path

# è·å–é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).parent.parent

def get_all_test_cases():
    """
    è·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    """
    try:
        # è¿è¡Œpytest --collect-onlyè·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        result = subprocess.run(
            ['pytest', '--collect-only', '-q'],
            cwd=ROOT_DIR,
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        
        if result.returncode != 0:
            print(f"Error running pytest: {result.stderr}")
            return []
        
        # è§£æè¾“å‡ºï¼Œæå–æµ‹è¯•ç”¨ä¾‹
        test_cases = []
        for line in result.stdout.split('\n'):
            if '::' in line and not line.startswith('='):
                test_cases.append(line.strip())
        
        return test_cases
    
    except Exception as e:
        print(f"Error getting test cases: {e}")
        return []

def analyze_test_cases(test_cases):
    """
    åˆ†ææµ‹è¯•ç”¨ä¾‹
    """
    analysis = {
        'total_count': len(test_cases),
        'by_file': defaultdict(list),
        'by_class': defaultdict(list),
        'by_category': defaultdict(list),
        'by_functionality': defaultdict(list)
    }
    
    for test_case in test_cases:
        # è§£ææµ‹è¯•ç”¨ä¾‹è·¯å¾„
        parts = test_case.split('::')
        if len(parts) >= 2:
            file_path = parts[0]
            test_name = parts[-1]
            
            # æŒ‰æ–‡ä»¶åˆ†ç±»
            analysis['by_file'][file_path].append(test_case)
            
            # æŒ‰ç±»åˆ†ç±»ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if len(parts) >= 3:
                class_name = parts[1]
                analysis['by_class'][class_name].append(test_case)
            
            # æŒ‰åŠŸèƒ½åˆ†ç±»
            functionality = categorize_by_functionality(file_path, test_name)
            analysis['by_functionality'][functionality].append(test_case)
            
            # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç±»
            category = categorize_by_type(file_path)
            analysis['by_category'][category].append(test_case)
    
    return analysis

def categorize_by_functionality(file_path, test_name):
    """
    æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œæµ‹è¯•åç§°æ¨æ–­åŠŸèƒ½ç±»åˆ«
    """
    file_path_lower = file_path.lower()
    test_name_lower = test_name.lower()
    
    # APIç›¸å…³æµ‹è¯•
    if 'api' in file_path_lower or 'endpoint' in test_name_lower:
        return 'APIæ¥å£æµ‹è¯•'
    
    # æ£€æµ‹å™¨ç›¸å…³æµ‹è¯•
    if 'detector' in file_path_lower:
        if 'hairnet' in file_path_lower:
            return 'å‘ç½‘æ£€æµ‹æµ‹è¯•'
        elif 'pose' in file_path_lower:
            return 'å§¿æ€æ£€æµ‹æµ‹è¯•'
        else:
            return 'é€šç”¨æ£€æµ‹æµ‹è¯•'
    
    # è¡Œä¸ºè¯†åˆ«æµ‹è¯•
    if 'behavior' in file_path_lower or 'handwash' in test_name_lower:
        return 'è¡Œä¸ºè¯†åˆ«æµ‹è¯•'
    
    # è¿åŠ¨åˆ†ææµ‹è¯•
    if 'motion' in file_path_lower:
        return 'è¿åŠ¨åˆ†ææµ‹è¯•'
    
    # æ•°å­¦å·¥å…·æµ‹è¯•
    if 'math' in file_path_lower:
        return 'æ•°å­¦å·¥å…·æµ‹è¯•'
    
    # æ•°æ®ç®¡ç†æµ‹è¯•
    if 'data_manager' in file_path_lower:
        return 'æ•°æ®ç®¡ç†æµ‹è¯•'
    
    # GPUé…ç½®æµ‹è¯•
    if 'gpu' in file_path_lower or 'mediapipe' in file_path_lower:
        return 'GPUé…ç½®æµ‹è¯•'
    
    # é˜ˆå€¼è°ƒæ•´æµ‹è¯•
    if 'threshold' in file_path_lower:
        return 'é˜ˆå€¼è°ƒæ•´æµ‹è¯•'
    
    return 'å…¶ä»–æµ‹è¯•'

def categorize_by_type(file_path):
    """
    æ ¹æ®æ–‡ä»¶è·¯å¾„æ¨æ–­æµ‹è¯•ç±»å‹
    """
    if 'unit' in file_path:
        return 'å•å…ƒæµ‹è¯•'
    elif 'integration' in file_path:
        return 'é›†æˆæµ‹è¯•'
    elif file_path.startswith('test_'):
        return 'æ ¹ç›®å½•æµ‹è¯•'
    else:
        return 'å…¶ä»–æµ‹è¯•'

def print_analysis(analysis):
    """
    æ‰“å°åˆ†æç»“æœ
    """
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•ç”¨ä¾‹åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print(f"æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {analysis['total_count']}")
    
    # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç±»
    print(f"\nğŸ“ æŒ‰æµ‹è¯•ç±»å‹åˆ†ç±»:")
    for category, tests in sorted(analysis['by_category'].items()):
        print(f"  {category}: {len(tests)}ä¸ª")
    
    # æŒ‰åŠŸèƒ½åˆ†ç±»
    print(f"\nğŸ”§ æŒ‰åŠŸèƒ½æ¨¡å—åˆ†ç±»:")
    for functionality, tests in sorted(analysis['by_functionality'].items(), key=lambda x: len(x[1]), reverse=True):
        print(f"  {functionality}: {len(tests)}ä¸ª")
    
    # æŒ‰æ–‡ä»¶åˆ†ç±»
    print(f"\nğŸ“„ æŒ‰æµ‹è¯•æ–‡ä»¶åˆ†ç±»:")
    for file_path, tests in sorted(analysis['by_file'].items(), key=lambda x: len(x[1]), reverse=True):
        file_name = Path(file_path).name
        print(f"  {file_name}: {len(tests)}ä¸ªæµ‹è¯•")
    
    # æŒ‰æµ‹è¯•ç±»åˆ†ç±»
    print(f"\nğŸ·ï¸  æŒ‰æµ‹è¯•ç±»åˆ†ç±»:")
    class_counts = Counter()
    for class_name, tests in analysis['by_class'].items():
        class_counts[class_name] = len(tests)
    
    for class_name, count in class_counts.most_common(10):  # æ˜¾ç¤ºå‰10ä¸ªæœ€å¤šçš„ç±»
        print(f"  {class_name}: {count}ä¸ªæµ‹è¯•")
    
    # è¯¦ç»†åŠŸèƒ½åˆ†æ
    print(f"\nğŸ” è¯¦ç»†åŠŸèƒ½åˆ†æ:")
    for functionality, tests in sorted(analysis['by_functionality'].items()):
        print(f"\n  ğŸ“Œ {functionality} ({len(tests)}ä¸ª):")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤º
        file_groups = defaultdict(list)
        for test in tests:
            file_path = test.split('::')[0]
            file_name = Path(file_path).name
            file_groups[file_name].append(test)
        
        for file_name, file_tests in sorted(file_groups.items()):
            print(f"    ğŸ“„ {file_name}: {len(file_tests)}ä¸ª")
            
            # æ˜¾ç¤ºå…·ä½“çš„æµ‹è¯•æ–¹æ³•ï¼ˆé™åˆ¶æ˜¾ç¤ºæ•°é‡ï¼‰
            for test in file_tests[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                test_method = test.split('::')[-1]
                print(f"      - {test_method}")
            
            if len(file_tests) > 3:
                print(f"      - ... è¿˜æœ‰{len(file_tests) - 3}ä¸ªæµ‹è¯•")

def generate_test_summary():
    """
    ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ€»ç»“
    """
    print("æ­£åœ¨æ”¶é›†æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯...")
    test_cases = get_all_test_cases()
    
    if not test_cases:
        print("æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹")
        return
    
    print(f"æ‰¾åˆ° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # åˆ†ææµ‹è¯•ç”¨ä¾‹
    analysis = analyze_test_cases(test_cases)
    
    # æ‰“å°åˆ†æç»“æœ
    print_analysis(analysis)
    
    # ç”Ÿæˆæµ‹è¯•è¦†ç›–å»ºè®®
    print(f"\nğŸ’¡ æµ‹è¯•è¦†ç›–å»ºè®®:")
    
    functionality_counts = {k: len(v) for k, v in analysis['by_functionality'].items()}
    
    # æ‰¾å‡ºæµ‹è¯•è¾ƒå°‘çš„åŠŸèƒ½æ¨¡å—
    low_coverage = [func for func, count in functionality_counts.items() if count < 5]
    if low_coverage:
        print(f"  ä»¥ä¸‹åŠŸèƒ½æ¨¡å—æµ‹è¯•ç”¨ä¾‹è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æµ‹è¯•:")
        for func in low_coverage:
            print(f"    - {func}: {functionality_counts[func]}ä¸ªæµ‹è¯•")
    
    # æµ‹è¯•åˆ†å¸ƒå»ºè®®
    unit_tests = len(analysis['by_category'].get('å•å…ƒæµ‹è¯•', []))
    integration_tests = len(analysis['by_category'].get('é›†æˆæµ‹è¯•', []))
    
    print(f"\n  æµ‹è¯•ç±»å‹åˆ†å¸ƒ:")
    print(f"    - å•å…ƒæµ‹è¯•: {unit_tests}ä¸ª ({unit_tests/analysis['total_count']*100:.1f}%)")
    print(f"    - é›†æˆæµ‹è¯•: {integration_tests}ä¸ª ({integration_tests/analysis['total_count']*100:.1f}%)")
    
    if unit_tests / analysis['total_count'] < 0.7:
        print(f"  âš ï¸  å»ºè®®å¢åŠ æ›´å¤šå•å…ƒæµ‹è¯•ï¼Œç›®å‰å•å…ƒæµ‹è¯•å æ¯”è¾ƒä½")
    
    if integration_tests / analysis['total_count'] < 0.2:
        print(f"  âš ï¸  å»ºè®®å¢åŠ æ›´å¤šé›†æˆæµ‹è¯•ï¼Œç¡®ä¿æ¨¡å—é—´äº¤äº’æ­£å¸¸")

if __name__ == "__main__":
    generate_test_summary()